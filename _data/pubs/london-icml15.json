{
    "type": "conference",
    "title": "The Benefits of Learning with Strongly Convex Approximate Inference",
    "authors": [
        "Ben London",
        "Bert Huang",
        "Lise Getoor"
    ],
    "venue": "International Conference on Machine Learning",
    "year": "2015",
    "publisher": "PMLR",
    "address": "Lille, France",
    "links": [
        {
            "label": "paper",
            "href": "/assets/resources/london-icml15.pdf"
        }
    ],
    "abstract": "We explore the benefits of strongly convex free energies in variational inference, providing both theoretical motivation and a new meta-algorithm. Using the duality between strong convexity and stability, we prove a high-probability bound on the error of learned marginals that is inversely proportional to the modulus of convexity of the free energy, thereby motivating free energies whose moduli are constant with respect to the size of the graph. We identify sufficient conditions for \u03a9(1)-strong convexity in two popular variational techniques: tree-reweighted and counting number entropies. Our insights for the latter suggest a novel counting number optimization framework, which guarantees strong convexity for any given modulus. Our experiments demonstrate that learning with a strongly convex free energy, using our optimization framework to guarantee a given modulus, results in substantially more accurate marginal probabilities, thereby validating our theoretical claims and the effectiveness of our framework."
}
