{
  "type": "conference",
  "title": "Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic",
  "authors": [
    "Connor Pryor",
    "Quan Yuan",
    "Jeremiah Zhe Liu",
    "Seyed Mehran Kazemi",
    "Deepak Ramachandran",
    "Tania Bedrax-Weiss",
    "Lise Getoor"
  ],
  "venue": "ICLR Workshop on Neurosymbolic Generative Models",
  "year": "2023",
  "publisher": "ICLR",
  "address": "Kigali, Rwanda",
  "links": [
  ],
  "abstract": "Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialogue system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialog Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. Over three unsupervised dialog structure induction datasets the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.",
  "keywords": [
    "dialog structure induction",
    "dsi",
    "neupsl",
    "dd-vrnn",
    "neuro-symbolic",
    "neuro-symbolic computing",
    "nesy",
    "graphical models",
    "energy-based models",
    "srl",
    "psl",
    "application"
  ]
}
